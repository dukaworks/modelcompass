import axios from 'axios';
import { prisma } from '../utils/db';

interface HuggingFaceModel {
  id: string;
  modelId: string;
  name: string;
  description?: string;
  tags: string[];
  downloads: number;
  likes: number;
  pipeline_tag?: string;
  license?: string;
  language?: string[];
  url: string;
}

export class HuggingFaceCrawler {
  private baseUrl = 'https://huggingface.co/api/models';

  /**
   * æŠ“å– LLM æ¨¡å‹åˆ—è¡¨
   */
  async crawlLLMs(limit = 100): Promise<HuggingFaceModel[]> {
    console.log(`ğŸ•·ï¸ ä» HuggingFace æŠ“å– LLM æ¨¡å‹ (limit: ${limit})...`);
    
    try {
      // HF API æ”¯æŒç­›é€‰ text-generation ä»»åŠ¡
      const response = await axios.get(this.baseUrl, {
        params: {
          filter: 'text-generation',
          sort: 'downloads',
          direction: -1,
          limit: limit
        },
        timeout: 30000
      });

      const models: HuggingFaceModel[] = response.data.map((m: any) => ({
        id: m._id,
        modelId: m.modelId,
        name: m.modelId.split('/').pop(),
        description: this.extractDescription(m.cardData),
        tags: m.tags || [],
        downloads: m.downloads || 0,
        likes: m.likes || 0,
        pipeline_tag: m.pipeline_tag,
        license: m.license,
        language: m.cardData?.language || m.tags?.filter((t: string) => 
          ['zh', 'en', 'multilingual', 'chinese'].includes(t)
        ),
        url: `https://huggingface.co/${m.modelId}`
      }));

      console.log(`âœ… æŠ“å–åˆ° ${models.length} ä¸ª HF æ¨¡å‹`);
      return models;
    } catch (error: any) {
      console.error('âŒ HuggingFace æŠ“å–å¤±è´¥:', error.message);
      return [];
    }
  }

  /**
   * æŠ“å–ç‰¹å®šæ¨¡å‹è¯¦æƒ…
   */
  async crawlModelDetail(modelId: string): Promise<Partial<HuggingFaceModel>> {
    try {
      const response = await axios.get(`${this.baseUrl}/${modelId}`, {
        timeout: 10000
      });

      const m = response.data;
      return {
        description: this.extractDescription(m.cardData),
        tags: m.tags || [],
        downloads: m.downloads || 0,
        likes: m.likes || 0,
        license: m.license,
        language: m.cardData?.language
      };
    } catch (error: any) {
      console.error(`âŒ æŠ“å–è¯¦æƒ…å¤±è´¥ ${modelId}:`, error.message);
      return {};
    }
  }

  /**
   * æå–æè¿°
   */
  private extractDescription(cardData: any): string | undefined {
    if (!cardData) return undefined;
    
    const desc = cardData.description || 
                 cardData.abstract || 
                 cardData.model_summary ||
                 (typeof cardData === 'string' ? cardData : undefined);
    
    return desc?.substring(0, 500);
  }

  /**
   * è½¬æ¢ä¸ºé­”ç›˜ Model æ ¼å¼
   */
  convertToModel(hfModel: HuggingFaceModel): any {
    const provider = this.inferProvider(hfModel.modelId);
    
    return {
      modelId: `hf-${hfModel.modelId.replace(/\//g, '-')}`,
      name: hfModel.name,
      provider: provider,
      description: hfModel.description || `HuggingFace æ¨¡å‹ï¼š${hfModel.modelId}`,
      capabilities: this.inferCapabilities(hfModel),
      contextLength: this.inferContextLength(hfModel),
      promptPrice: 0,  // HF å…è´¹ä½¿ç”¨
      completionPrice: 0,
      tags: [
        ...hfModel.tags?.slice(0, 5) || [],
        'å¼€æº',
        'å…è´¹',
        'HuggingFace',
        hfModel.downloads > 100000 ? 'çƒ­é—¨' : '',
        hfModel.likes > 500 ? 'é«˜åˆ†' : ''
      ].filter(Boolean),
      recommendedFor: this.inferRecommendations(hfModel),
      websiteUrl: hfModel.url,
      docsUrl: `${hfModel.url}#model-card`,
      isActive: true
    };
  }

  private inferProvider(modelId: string): string {
    if (modelId.includes('meta-llama')) return 'meta';
    if (modelId.includes('mistral')) return 'mistral';
    if (modelId.includes('Qwen')) return 'alibaba';
    if (modelId.includes('deepseek')) return 'deepseek';
    if (modelId.includes('google')) return 'google';
    if (modelId.includes('microsoft')) return 'microsoft';
    return 'community';
  }

  private inferCapabilities(model: HuggingFaceModel): string[] {
    const caps = ['chat'];
    const tags = model.tags?.map(t => t.toLowerCase()) || [];
    
    if (tags.some(t => t.includes('vision') || t.includes('image'))) caps.push('vision');
    if (tags.some(t => t.includes('code') || t.includes('programming'))) caps.push('code');
    if (model.pipeline_tag === 'text-generation') caps.push('generation');
    
    return caps;
  }

  private inferContextLength(model: HuggingFaceModel): number {
    const tags = model.tags?.join(' ') || '';
    
    if (tags.includes('32k') || tags.includes('32768')) return 32768;
    if (tags.includes('128k') || tags.includes('131072')) return 131072;
    if (tags.includes('4k') || tags.includes('4096')) return 4096;
    if (tags.includes('8k') || tags.includes('8192')) return 8192;
    
    return 4096;
  }

  private inferRecommendations(model: HuggingFaceModel): string[] {
    const recs = ['æœ¬åœ°éƒ¨ç½²', 'éšç§ä¼˜å…ˆ'];
    const tags = model.tags?.map(t => t.toLowerCase()) || [];
    
    if (model.downloads > 1000000) recs.push('ç¤¾åŒºçƒ­é—¨');
    if (tags.some(t => t.includes('chinese') || t.includes('zh'))) recs.push('ä¸­æ–‡æ”¯æŒ');
    if (model.license === 'apache-2.0' || model.license === 'mit') recs.push('å•†ä¸šå‹å¥½');
    
    return recs;
  }

  /**
   * è¿è¡Œå®Œæ•´æŠ“å–
   */
  async run(): Promise<void> {
    console.log('ğŸš€ å¯åŠ¨ HuggingFace çˆ¬è™«...');
    
    const models = await this.crawlLLMs(100);
    
    for (const hfModel of models.slice(0, 50)) {
      try {
        const modelData = this.convertToModel(hfModel);
        
        await prisma.model.upsert({
          where: { modelId: modelData.modelId },
          update: modelData,
          create: modelData
        });
        
        console.log(`âœ… å·²ä¿å­˜: ${modelData.name}`);
      } catch (error: any) {
        console.error(`âŒ ä¿å­˜å¤±è´¥ ${hfModel.modelId}:`, error.message);
      }
    }
    
    console.log('\nğŸ‰ HuggingFace çˆ¬è™«å®Œæˆï¼');
  }
}

if (require.main === module) {
  const crawler = new HuggingFaceCrawler();
  crawler.run().catch(console.error);
}
